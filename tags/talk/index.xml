<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>talk on sgarzare&#39;s blog</title>
    <link>https://stefano-garzarella.github.io/tags/talk/</link>
    <description>Recent content in talk on sgarzare&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>sgarzare@redhat.com (Stefano Garzarella)</managingEditor>
    <webMaster>sgarzare@redhat.com (Stefano Garzarella)</webMaster>
    <lastBuildDate>Tue, 25 Feb 2020 20:30:21 +0100</lastBuildDate>
    
        <atom:link href="https://stefano-garzarella.github.io/tags/talk/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>AF_VSOCK: nested VMs and loopback support available</title>
      <link>https://stefano-garzarella.github.io/posts/2020-02-20-vsock-nested-vms-loopback/</link>
      <pubDate>Tue, 25 Feb 2020 20:30:21 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2020-02-20-vsock-nested-vms-loopback/</guid>
      <description>&lt;p&gt;During the last
&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/&#34;&gt;KVM Forum 2019&lt;/a&gt;,
we discussed some next steps and several requests came from the audience.&lt;/p&gt;
&lt;p&gt;In the last months, we worked on that and recent Linux releases contain
interesting new features that we will describe in this blog post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nested-vms&#34;&gt;Nested VMs support&lt;/a&gt;, available in Linux 5.5&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local-communication&#34;&gt;Local communication support&lt;/a&gt;, available in Linux 5.6&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;devconfcz-2020&#34;&gt;DevConf.CZ 2020&lt;/h2&gt;
&lt;p&gt;These updates and an introduction to AF_VSOCK ware presented at
&lt;strong&gt;DevConf.CZ 2020&lt;/strong&gt; during the
&amp;ldquo;&lt;a href=&#34;https://devconfcz2020a.sched.com/event/YOwb/vsock-vm-host-socket-with-minimal-configuration&#34;&gt;VSOCK: VM↔host socket with minimal configuration&lt;/a&gt;&amp;rdquo; talk.
Slides available &lt;a href=&#34;https://static.sched.com/hosted_files/devconfcz2020a/b1/DevConf.CZ_2020_vsock_v1.1.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;nested-vms&#34;&gt;Nested VMs&lt;/h2&gt;
&lt;p&gt;Before Linux 5.5, the AF_VSOCK core supported only one transport loaded at
run time. That was a limit for nested VMs, because we need multiple transports
loaded together.&lt;/p&gt;
&lt;h3 id=&#34;types-of-transport&#34;&gt;Types of transport&lt;/h3&gt;
&lt;p&gt;Under the AF_VSOCK core, that provides the socket interface to the user space
applications, we have several transports that implement the communication
channel between guest and host.&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://stefano-garzarella.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock transports&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These transports depend on the hypervisor and we can put them in two groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H2G&lt;/strong&gt; (host to guest) transports: they run in the host and
usually they provide the device emulation; currently we have &lt;em&gt;vhost&lt;/em&gt; and &lt;em&gt;vmci&lt;/em&gt;
transports.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G2H&lt;/strong&gt; (guest to host) transports: they run in the guest and usually they are
device drivers; currently we have &lt;em&gt;virtio, vmci,&lt;/em&gt; and &lt;em&gt;hyperv&lt;/em&gt; transports.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-transports&#34;&gt;Multi-transports&lt;/h3&gt;
&lt;p&gt;In a nested VM environment, we need to load both G2H and H2G transports
together in the L1 guest, for this reason, we implemented the multi-transports
support to use vsock through &lt;strong&gt;nested VMs&lt;/strong&gt;.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock and nested VMs&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Starting from Linux 5.5, the AF_VSOCK can handle two types of transports
loaded together at runtime:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H2G&lt;/strong&gt; transport, to communicate with the guest&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G2H&lt;/strong&gt; transport, to communicate with the host.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So in the QEMU/KVM environment, the L1 guest will load both &lt;em&gt;virtio-transport&lt;/em&gt;,
to communicate with L0, and &lt;em&gt;vhost-transport&lt;/em&gt; to communicate with L2.&lt;/p&gt;
&lt;h2 id=&#34;local-communication&#34;&gt;Local Communication&lt;/h2&gt;
&lt;p&gt;Another feature recently added is the possibility to communicate locally on
the same host.
This feature, suggested by &lt;a href=&#34;https://rwmj.wordpress.com/&#34;&gt;Richard WM Jones&lt;/a&gt;,
can be very useful for testing and debugging applications that use AF_VSOCK
without running VMs.&lt;/p&gt;
&lt;p&gt;Linux 5.6 introduces a new transport called &lt;em&gt;vsock-loopback&lt;/em&gt;, and a new well
know CID for local communication: &lt;strong&gt;VMADDR_CID_LOCAL (1)&lt;/strong&gt;.
It’s a special CID to direct packets to the same host that generated them.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock loopback&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Other CIDs can be used for the same purpose, but it&amp;rsquo;s preferable to use
VMADDR_CID_LOCAL:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local Guest CID
&lt;ul&gt;
&lt;li&gt;if G2H is loaded (e.g. running in a VM)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VMADDR_CID_HOST (2)
&lt;ul&gt;
&lt;li&gt;if H2G is loaded and G2H is not loaded (e.g. running on L0).
If G2H is also loaded, then VMADDR_CID_HOST is used to reach the host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Richard recently used the vsock local communication to implement a &lt;a href=&#34;https://github.com/libguestfs/nbdkit/commit/5e4745641bb4676f607fdb3f8750dbf6e9516877&#34;&gt;regression
test&lt;/a&gt;
test for &lt;a href=&#34;https://rwmj.wordpress.com/2019/10/21/nbd-over-af_vsock/&#34;&gt;nbdkit/libnbd vsock support&lt;/a&gt;, using the new &lt;a href=&#34;https://github.com/libguestfs/nbdkit/commit/01ce88f56f93afd577a8e3b2cc28d825693c8db2&#34;&gt;VMADDR_CID_LOCAL&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Listening on port 1234 using ncat(1)&lt;/span&gt;
l0$ nc --vsock -l &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;# Connecting to the local host using VMADDR_CID_LOCAL (1)&lt;/span&gt;
l0$ nc --vsock &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;patches&#34;&gt;Patches&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1194660/&#34;&gt;[PATCH net-next v2 00/15] vsock: add multi-transports support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1207012/&#34;&gt;[PATCH net-next v2 0/6] vsock: add local transport support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>KVM Forum 2019: virtio-vsock in QEMU, Firecracker and Linux</title>
      <link>https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/</link>
      <pubDate>Sat, 09 Nov 2019 18:45:25 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://static.sched.com/hosted_files/kvmforum2019/50/KVMForum_2019_virtio_vsock_Andra_Paraschiv_Stefano_Garzarella_v1.3.pdf&#34;&gt;Slides&lt;/a&gt; and
&lt;a href=&#34;https://youtu.be/LFqz-VZPhFE&#34;&gt;recording&lt;/a&gt; are available for the &amp;ldquo;&lt;a href=&#34;https://sched.co/TmwK&#34;&gt;virtio-vsock in QEMU,
Firecracker and Linux: Status, Performance and Challenges&lt;/a&gt;&amp;rdquo;
talk that Andra Paraschiv and I presented at
&lt;a href=&#34;https://events19.linuxfoundation.org/events/kvm-forum-2019/&#34;&gt;KVM Forum 2019&lt;/a&gt;.
This was the 13th edition of the KVM Forum conference. It took place in Lyon,
France in October 2019.&lt;/p&gt;
&lt;p&gt;We talked about the current status and future works of VSOCK drivers in Linux
and how Firecracker and QEMU provides the virtio-vsock device.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Initially, Andra gave an overview of VSOCK, she described the state of the art,
and the key features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;it is very simple to configure, the host assigns an unique CID (Context-ID) to
each guest, and no configuration is needed inside the guest;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;it provides AF_VSOCK address family, allowing user space application in the
host and guest to communicate using standard POSIX Socket API (e.g.
bind, listen, accept, connect, send, recv, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Andra also described common use cases for VSOCK, such as guest agents
(clipboard sharing, remote console, etc.), network applications using
SOCK_STREAM, and services provided by the hypervisor to the guests.&lt;/p&gt;
&lt;p&gt;Going into the implementation details, Andra explained how the device in the
guest communicates with the vhost backend in the host, exchanging data and
events (i.e. ioeventfd, irqfd).&lt;/p&gt;
&lt;h3 id=&#34;firecracker&#34;&gt;Firecracker&lt;/h3&gt;
&lt;p&gt;Focusing on Firecracker, Andra gave a brief overview on this new VMM (Virtual
Machine Monitor) written in Rust and she explained why, in the v0.18.0 release,
they switched from the experimental vhost-vsock implementation to a vhost-less
solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus on security impact&lt;/li&gt;
&lt;li&gt;less dependency on host kernel features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This change required a device emulation in Firecracker, that implements
virtio-vsock device model over MMIO. The device is exposed in the host using
UDS (Unix Domain Sockets).&lt;/p&gt;
&lt;p&gt;Andra described how Firecracker maps the VSOCK ports on the &lt;code&gt;uds_path&lt;/code&gt; specified
in the VM configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Host-Initiated Connections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guest: create an AF_VSOCK socket and &lt;code&gt;listen()&lt;/code&gt; on PORT&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;connect()&lt;/code&gt; to AF_UNIX at &lt;code&gt;uds_path&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;send()&lt;/code&gt; &amp;ldquo;CONNECT PORT\n&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Guest: &lt;code&gt;accept()&lt;/code&gt; the new connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guest-Initiated Connections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Host: create and &lt;code&gt;listen()&lt;/code&gt; on an AF_UNIX socket at &lt;code&gt;uds_path_PORT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Guest: create an AF_VSOCK socket and &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;HOST_CID&lt;/code&gt; and &lt;code&gt;PORT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;accept()&lt;/code&gt; the new connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, she showed the performance of this solution, running
&lt;a href=&#34;https://github.com/stefano-garzarella/iperf-vsock&#34;&gt;iperf-vsock&lt;/a&gt; benchmark,
varying the size of the buffer used in Firecracker to transfer packets
between the virtio-vsock device and the UNIX domain socket. The throughput
on the guest to host path reaches 10 Gbps.&lt;/p&gt;
&lt;h3 id=&#34;qemu&#34;&gt;QEMU&lt;/h3&gt;
&lt;p&gt;In the second part of the talk, I described the QEMU implementation.
QEMU provides the virtio-vsock device using the vhost-vsock kernel module.&lt;/p&gt;
&lt;p&gt;The vsock device in QEMU handles only:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;configuration
&lt;ul&gt;
&lt;li&gt;user or management tool can configure the guest CID&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;live-migration
&lt;ul&gt;
&lt;li&gt;connected SOCK_STREAM sockets become disconnected.
Applications must handle a connection reset error and should reconnect.&lt;/li&gt;
&lt;li&gt;guest CID can be not available in the new host because can be
assigned to another VM. In this case the guest is notified about the
CID change.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vhost-vsock kernel module handles the communication with the guest,
providing in-kernel virtio device emulation, to have very high performance and
to interface directly to the host socket layer.
In this way, also host application can directly use POSIX Socket API to
communicate with the guest. So, guest and host applications can be switched
between them, changing only the destination CID.&lt;/p&gt;
&lt;h3 id=&#34;virtio-vsock-linux-drivers&#34;&gt;virtio-vsock Linux drivers&lt;/h3&gt;
&lt;p&gt;After that, I told the story of VSOCK in the Linux tree, started in 2013
when the first implementation was merged, and the changes in the last year.&lt;/p&gt;
&lt;p&gt;These changes mainly regard fixes, but for the virtio/vhost transports we also
improved the performance with two simple changes released with Linux v5.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reducing the number of credit update messages exchanged&lt;/li&gt;
&lt;li&gt;increasing the size of packets queued in the virtio-vsock device from 4 KB up
to 64 KB, the maximum packet size handled by virtio-vsock devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these changes we are able to reach ~40 Gbps in the Guest -&amp;gt; Host path,
because the guest can now send up to 64 KB packets directly to the host;
for the Host -&amp;gt; Guest path, we reached ~25 Gbps, because the host is still
using 4 KB buffer preallocated by the guest.&lt;/p&gt;
&lt;h3 id=&#34;tools-and-languages-that-support-vsock&#34;&gt;Tools and languages that support VSOCK&lt;/h3&gt;
&lt;p&gt;In the last few years, several applications, tools, and languages started to
support VSOCK and I listed them to update the audience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;wireshark &amp;gt;= 2.40 [2017-07-19]&lt;/li&gt;
&lt;li&gt;iproute2 &amp;gt;= 4.15 [2018-01-28]
&lt;ul&gt;
&lt;li&gt;ss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tcpdump
&lt;ul&gt;
&lt;li&gt;merged in master [2019-04-16]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nmap &amp;gt;= 7.80 [2019-08-10]
&lt;ul&gt;
&lt;li&gt;ncat&lt;/li&gt;
&lt;li&gt;nbd&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nbdkit &amp;gt;= 1.15.5 [2019-10-19]&lt;/li&gt;
&lt;li&gt;libnbd &amp;gt;= 1.1.6 [2019-10-19]&lt;/li&gt;
&lt;li&gt;iperf-vsock
&lt;ul&gt;
&lt;li&gt;iperf3 fork&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Languages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C
&lt;ul&gt;
&lt;li&gt;glibc &amp;gt;= 2.18 [2013-08-10]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python
&lt;ul&gt;
&lt;li&gt;python &amp;gt;= 3.7 alpha 1 [2017-09-19]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Golang
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mdlayher/vsock&#34;&gt;https://github.com/mdlayher/vsock&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rust
&lt;ul&gt;
&lt;li&gt;libc crate &amp;gt;= 0.2.59 [2019-07-08]
&lt;ul&gt;
&lt;li&gt;struct sockaddr_vm&lt;/li&gt;
&lt;li&gt;VMADDR_* macros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nix crate &amp;gt;= 0.15.0 [2019-08-10]
&lt;ul&gt;
&lt;li&gt;VSOCK supported in the socket API (nix::sys::socket)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;next-steps&#34;&gt;Next steps&lt;/h3&gt;
&lt;p&gt;Concluding, I went through the next challenges that we are going to face:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;multi-transport&lt;/strong&gt; to use VSOCK in a nested VM environment. because we are
limited by the fact that the current implementation can handle only one
transport loaded at run time, so, we can&amp;rsquo;t load virtio_transport and
vhost_transport together in the L1 guest.
I already sent some patches upstream
[&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1168442/&#34;&gt;RFC&lt;/a&gt;,
&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1181986/&#34;&gt;v1&lt;/a&gt;],
but they are still in progress.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;network namespace support&lt;/strong&gt; to create independent addressing domains with
VSOCK socket. This could be useful for partitioning VMs in different
domains or, in a nested VM environment, to isolate host applications
from guest applications bound to the same port.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;virtio-net as a transport for the virtio-vsock&lt;/strong&gt; to avoid to re-implement
features already done in virtio-net, such as mergeable buffers,
page allocation, small packet handling.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;from-the-audience&#34;&gt;From the audience&lt;/h4&gt;
&lt;p&gt;Other points to be addressed came from the comments we received from the
audience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;loopback device&lt;/strong&gt; could be very useful for developers to test applications
that use VSOCK socket. The current implementation support loopback only
in the guest, but it would be better to support it also in the host, adding
&lt;code&gt;VMADDR_CID_LOCAL&lt;/code&gt; special address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VM to VM communication&lt;/strong&gt; was asked by several people. Introducing it in the
VSOCK core could complicate the protocol, the addressing and could require
some sort of firewall.
For now we do not have in mind to do it, but I developed a simple user
space application to solve this issue:
&lt;a href=&#34;https://github.com/stefano-garzarella/vsock-bridge&#34;&gt;vsock-bridge&lt;/a&gt;.
In order to improve the performance of this solution, we will consider
the possibility to add &lt;code&gt;sendfile(2)&lt;/code&gt; or
&lt;a href=&#34;https://www.kernel.org/doc/html/v4.15/networking/msg_zerocopy.html&#34;&gt;MSG_ZEROCOPY&lt;/a&gt;
support to the AF_VSOCK core.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;virtio-vsock windows drivers&lt;/strong&gt; is not planned to be addressed, but contributions
are welcome. Other virtio windows drivers are available in the
&lt;a href=&#34;https://github.com/virtio-win/kvm-guest-drivers-windows&#34;&gt;vm-guest-drivers-windows&lt;/a&gt;
repository.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
