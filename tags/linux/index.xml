<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>linux on sgarzare&#39;s blog</title>
    <link>https://stefano-garzarella.github.io/tags/linux/</link>
    <description>Recent content in linux on sgarzare&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>sgarzare@redhat.com (Stefano Garzarella)</managingEditor>
    <webMaster>sgarzare@redhat.com (Stefano Garzarella)</webMaster>
    <lastBuildDate>Mon, 12 Feb 2024 18:42:57 +0100</lastBuildDate>
    
        <atom:link href="https://stefano-garzarella.github.io/tags/linux/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>vDPA: support for block devices in Linux and QEMU</title>
      <link>https://stefano-garzarella.github.io/posts/2024-02-12-vdpa-blk/</link>
      <pubDate>Mon, 12 Feb 2024 18:42:57 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2024-02-12-vdpa-blk/</guid>
      <description>&lt;p&gt;A &lt;em&gt;vDPA device&lt;/em&gt; is a type of device that follows the &lt;strong&gt;&lt;a href=&#34;https://docs.oasis-open.org/virtio/virtio/v1.3/virtio-v1.3.html&#34;&gt;virtio specification&lt;/a&gt;
for its datapath&lt;/strong&gt; but has a &lt;strong&gt;vendor-specific control path&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;vDPA devices can be both physically located on the hardware or emulated by
software.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://stefano-garzarella.github.io/img/vdpa_overview.png&#34; alt=&#34;vDPA overview&#34;&gt;&lt;/p&gt;
&lt;p&gt;A small vDPA parent driver in the host kernel is required only for the control
path. The main advantage is the &lt;strong&gt;unified software stack&lt;/strong&gt; for all vDPA
devices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;vhost interface&lt;/em&gt; (vhost-vdpa) for userspace or guest virtio driver, like a
VM running in QEMU&lt;/li&gt;
&lt;li&gt;&lt;em&gt;virtio interface&lt;/em&gt; (virtio-vdpa) for bare-metal or containerized applications
running in the host&lt;/li&gt;
&lt;li&gt;&lt;em&gt;management interface&lt;/em&gt; (vdpa netlink) for instantiating devices and
configuring virtio parameters&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;useful-resources&#34;&gt;Useful Resources&lt;/h3&gt;
&lt;p&gt;Many blog posts and talks have been published in recent years that can help you
better understand vDPA and the use cases. On &lt;a href=&#34;https://vdpa-dev.gitlab.io/&#34;&gt;vdpa-dev.gitlab.io&lt;/a&gt;
we collected some of them; I suggest you at least explore the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/en/blog/introduction-vdpa-kernel-framework&#34;&gt;Introduction to vDPA kernel framework&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.redhat.com/en/blog/introducing-vduse-software-defined-datapath-virtio&#34;&gt;Introducing VDUSE: a software-defined datapath for virtio&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;block-devices&#34;&gt;Block devices&lt;/h2&gt;
&lt;p&gt;Most of the work in vDPA has been driven by network devices, but in recent years,
we have also developed support for block devices.&lt;/p&gt;
&lt;p&gt;The main use case is definitely leveraging the hardware to directly emulate the
virtio-blk device and support different network backends such as Ceph RBD or
iSCSI. This is the goal of some SmartNICs or DPUs, which are able to emulate
virtio-net devices of course, but also virtio-blk for network storage.&lt;/p&gt;
&lt;p&gt;The abstraction provided by vDPA also makes software accelerators possible,
similar to existing vhost or vhost-user devices.
&lt;a href=&#34;https://kvmforum2021.sched.com/event/ke3a/vdpa-blk-unified-hardware-and-software-offload-for-virtio-blk-stefano-garzarella-red-hat&#34;&gt;We discussed about that at KVM Forum 2021&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We talked about the fast path and the slow path in that talk. When QEMU needs
to handle requests, like supporting live migration or executing I/O throttling,
it uses the slow path. During the slow path, the device exposed to the guest is
emulated in QEMU. QEMU intercepts the requests and forwards them to the vDPA
device by taking advantage of the driver implemented in libblkio.
On the other hand, when QEMU doesn&amp;rsquo;t need to intervene, the fast path comes
into play. In this case, the vDPA device can be directly exposed to the guest,
bypassing QEMU&amp;rsquo;s emulation.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gitlab.com/libblkio/libblkio&#34;&gt;libblkio&lt;/a&gt; exposes common API for accessing
block devices in userspace. It supports several drivers. We will focus more
on &lt;code&gt;virtio-blk-vhost-vdpa&lt;/code&gt; driver, which is used by &lt;code&gt;virtio-blk-vhost-vdpa&lt;/code&gt;
block device in QEMU. It only supports slow path for now, but in the future
it should be able to switch to fast path automatically. Since QEMU 7.2, it
supports libblkio drivers, so you can use the following options to attach a
vDPA block device to a VM:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   -blockdev node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive_src1,driver&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;virtio-blk-vhost-vdpa,path&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/vhost-vdpa-0,cache.direct&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;   -device virtio-blk-pci,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;src1,bootindex&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;2,drive&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive_src1 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Anyway, to fully leverage the performance of a vDPA hardware device, we can
always use the generic &lt;code&gt;vhost-vdpa-device-pci&lt;/code&gt; device offered by QEMU that
supports any vDPA device and exposes it directly to the guest. Of course,
QEMU is not able to intercept requests in this scenario and therefore some
features offered by its block layer (e.g. live migration, disk format, etc.)
are not supported. Since QEMU 8.0, you can use the following options to attach
a generic vDPA device to a VM:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    -device vhost-vdpa-device-pci,vhostdev&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/vhost-vdpa-0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At KVM Forum 2022, &lt;a href=&#34;https://kvmforum2022.sched.com/event/15jLs/introducing-the-libblkio-high-performance-block-io-api-stefan-hajnoczi-alberto-faria-red-hat&#34;&gt;Alberto Faria and Stefan Hajnoczi introduced libblkio&lt;/a&gt;,
while &lt;a href=&#34;https://kvmforum2022.sched.com/event/15jK5/qemu-storage-daemon-and-libblkio-exploring-new-shores-for-the-qemu-block-layer-kevin-wolf-stefano-garzarella-red-hat&#34;&gt;Kevin Wolf and I discussed its usage in the QEMU Storage Deamon (QSD)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;software-devices&#34;&gt;Software devices&lt;/h2&gt;
&lt;p&gt;One of the significant benefits of vDPA is its strong abstraction, enabling
the implementation of virtio devices in both hardware and softwareâ€”whether in
the kernel or user space. This unification under a single framework, where
devices appear identical for QEMU facilitates the seamless integration of
hardware and software components.&lt;/p&gt;
&lt;h3 id=&#34;kernel-devices&#34;&gt;Kernel devices&lt;/h3&gt;
&lt;p&gt;Regarding in-kernel devices, starting from Linux v5.13, there exists a simple
simulator designed for development and debugging purposes. It is available
through the &lt;code&gt;vdpa-sim-blk&lt;/code&gt; kernel module, which emulates a 128 MB ramdisk.
As highlighted in the presentation at KVM Forum 2021, a future device in the
kernel (similar to the repeatedly proposed but never merged &lt;code&gt;vhost-blk&lt;/code&gt;)
could potentially offer excellent performance. Such a device could be used
as an alternative when hardware is unavailable, for instance, facilitating
live migration in any system, regardless of whether the destination system
features a SmartNIC/DPU or not.&lt;/p&gt;
&lt;h3 id=&#34;user-space-devices&#34;&gt;User space devices&lt;/h3&gt;
&lt;p&gt;Instead, regarding user space, we can use VDUSE. QSD supports it and thus
allows us to export any disk image supported by QEMU, such as a vDPA device
in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qemu-storage-daemon &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --blockdev file,filename&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/path/to/disk.qcow2,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --blockdev qcow2,file&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;    --export &lt;span class=&#34;nv&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse-blk,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse0,name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse0,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2,writable&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;containers-vms-or-bare-metal&#34;&gt;Containers, VMs, or bare-metal&lt;/h2&gt;
&lt;p&gt;As mentioned in the introduction, vDPA supports different buses such as
&lt;code&gt;vhost-vdpa&lt;/code&gt; and &lt;code&gt;virtio-vdpa&lt;/code&gt;. This flexibility enables the utilization of
vDPA devices with virtual machines or user space drivers (e.g., libblkio)
through the &lt;code&gt;vhost-vdpa&lt;/code&gt; bus. Additionally, it allows interaction with
applications running directly on the host or within containers via the
&lt;code&gt;virtio-vdpa&lt;/code&gt; bus.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;vdpa&lt;/code&gt; tool in iproute2 facilitates the management of vdpa devices
through netlink, enabling the allocation and deallocation of these devices.&lt;/p&gt;
&lt;p&gt;Starting with Linux 5.17, vDPA drivers support &lt;code&gt;driver_ovveride&lt;/code&gt;. This
enhancement allows dynamic reconfiguration during runtime, permitting the
migration of a device from one bus to another in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# load vdpa buses&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ modprobe -a virtio-vdpa vhost-vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# load vdpa-blk in-kernel simulator&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ modprobe vdpa-sim-blk
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# instantiate a new vdpasim_blk device called `vdpa0`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ vdpa dev add mgmtdev vdpasim_blk name vdpa0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# `vdpa0` is attached to the first vDPA bus driver loaded&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ driverctl -b vdpa list-devices
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vdpa0 virtio_vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# change the `vdpa0` bus to `vhost-vdpa`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ driverctl -b vdpa set-override vdpa0 vhost_vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# `vdpa0` is now attached to the `vhost-vdpa` bus&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ driverctl -b vdpa list-devices
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vdpa0 vhost_vdpa &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;*&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Note: driverctl(8) integrates with udev so the binding is preserved.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Below are several examples on how to use &lt;em&gt;VDUSE&lt;/em&gt; and the &lt;em&gt;QEMU Storage Daemon&lt;/em&gt;
with VMs (&lt;code&gt;QEMU&lt;/code&gt;) or Containers (&lt;code&gt;podman&lt;/code&gt;).
These steps are easily adaptable to any hardware that supports virtio-blk
devices via vDPA.&lt;/p&gt;
&lt;h3 id=&#34;qcow2-image-available-for-host-applications-and-containers&#34;&gt;qcow2 image available for host applications and containers&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# load vdpa buses&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ modprobe -a virtio-vdpa vhost-vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# create an empty qcow2 image&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-img create -f qcow2 test.qcow2 10G
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# load vduse kernel module&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ modprobe vduse
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# launch QSD exposing the `test.qcow2` image as `vduse0` vDPA device&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-storage-daemon --blockdev file,filename&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;test.qcow2,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --blockdev qcow2,file&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --export vduse-blk,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse0,name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse0,num-queues&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2,writable&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# instantiate the `vduse0` device (same name used in QSD)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ vdpa dev add name vduse0 mgmtdev vduse
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# be sure to attach it to the `virtio-vdpa` device to use with host applications&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ driverctl -b vdpa set-override vduse0 virtio_vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# device exposed as a virtio device, but attached to the host kernel&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ lsblk -pv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME     TYPE TRAN   SIZE RQ-SIZE  MQ
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/vda disk virtio  10G     &lt;span class=&#34;m&#34;&gt;256&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# start a container with `/dev/vda` attached&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;podman run -it --rm --device /dev/vda --group-add keep-groups fedora:39 bash
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;launch-a-vm-using-a-vdpa-device&#34;&gt;Launch a VM using a vDPA device&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# download Fedora cloud image (or use any other bootable image you want)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ wget https://download.fedoraproject.org/pub/fedora/linux/releases/39/Cloud/x86_64/images/Fedora-Cloud-Base-39-1.5.x86_64.qcow2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# launch QSD exposing the VM image as `vduse1` vDPA device&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-storage-daemon &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --blockdev file,filename&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;Fedora-Cloud-Base-39-1.5.x86_64.qcow2,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --blockdev qcow2,file&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;file,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  --export vduse-blk,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse1,name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;vduse1,num-queues&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1,node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2,writable&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# instantiate the `vduse1` device (same name used in QSD)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ vdpa dev add name vduse1 mgmtdev vduse
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# initially it&amp;#39;s attached to the host (`/dev/vdb`), because `virtio-vdpa`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# is the first kernel module we loaded&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ lsblk -pv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME     TYPE TRAN   SIZE RQ-SIZE  MQ
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/vda disk virtio  10G     &lt;span class=&#34;m&#34;&gt;256&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/vdb disk virtio   5G     &lt;span class=&#34;m&#34;&gt;256&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ lsblk /dev/vdb
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;vdb    251:16   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;    5G  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; disk 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â”œâ”€vdb1 251:17   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;    1M  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; part 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â”œâ”€vdb2 251:18   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; 1000M  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; part 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â”œâ”€vdb3 251:19   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;  100M  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; part 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â”œâ”€vdb4 251:20   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;    4M  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; part 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;â””â”€vdb5 251:21   &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;  3.9G  &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; part 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# and it is identified as `virtio1` in the host&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ls /sys/bus/vdpa/devices/vduse1/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;driver  driver_override  power  subsystem  uevent  virtio1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# attach it to the `vhost-vdpa` device to use the device with VMs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ driverctl -b vdpa set-override vduse1 vhost_vdpa
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# `/dev/vdb` is not available anymore&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ lsblk -pv
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;NAME     TYPE TRAN   SIZE RQ-SIZE  MQ
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/dev/vda disk virtio  10G     &lt;span class=&#34;m&#34;&gt;256&lt;/span&gt;   &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# the device is identified as `vhost-vdpa-1` in the host&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ls /sys/bus/vdpa/devices/vduse1/
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;driver  driver_override  power  subsystem  uevent  vhost-vdpa-1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ ls -l /dev/vhost-vdpa-1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;crw-------. &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; root root 511, &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; Feb &lt;span class=&#34;m&#34;&gt;12&lt;/span&gt; 17:58 /dev/vhost-vdpa-1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# launch QEMU using `/dev/vhost-vdpa-1` device with the&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# `virtio-blk-vhost-vdpa` libblkio driver&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-system-x86_64 -m 512M -smp &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; -M q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm,memory-backend&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -object memory-backend-memfd,share&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem,size&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;512M&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -blockdev node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive0,driver&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;virtio-blk-vhost-vdpa,path&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/vhost-vdpa-1,cache.direct&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -device virtio-blk-pci,drive&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# `virtio-blk-vhost-vdpa` blockdev can be used with any QEMU block layer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# features (e.g live migration, I/O throttling).&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# In this example we are using I/O throttling:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-system-x86_64 -m 512M -smp &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; -M q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm,memory-backend&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -object memory-backend-memfd,share&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem,size&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;512M&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -blockdev node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive0,driver&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;virtio-blk-vhost-vdpa,path&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/vhost-vdpa-1,cache.direct&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -blockdev node-name&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;throttle0,driver&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;throttle,file&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;drive0,throttle-group&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;limits0 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -object throttle-group,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;limits0,x-iops-total&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2000&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -device virtio-blk-pci,drive&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;throttle0
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Alternatively, we can use the generic `vhost-vdpa-device-pci` to take&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# advantage of all the performance, but without having any QEMU block layer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# features available&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ qemu-system-x86_64 -m 512M -smp &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; -M q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm,memory-backend&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -object memory-backend-memfd,share&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;on,id&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;mem,size&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;512M&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -device vhost-vdpa-device-pci,vhostdev&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/dev/vhost-vdpa-0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>SOCAT now supports AF_VSOCK</title>
      <link>https://stefano-garzarella.github.io/posts/2021-01-22-socat-vsock/</link>
      <pubDate>Fri, 22 Jan 2021 15:16:04 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2021-01-22-socat-vsock/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.dest-unreach.org/isocat/&#34;&gt;SOCAT&lt;/a&gt;
is a CLI utility which enables the concatenation
of two sockets together.
It establishes two bidirectional byte streams and transfers data between them.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;socat&lt;/code&gt; supports several address types (e.g. TCP, UDP, UNIX domain sockets, etc.)
to construct the streams. The latest version &lt;strong&gt;1.7.4&lt;/strong&gt;, released earlier this
year [2021-01-04], supports also AF_VSOCK addresses:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.dest-unreach.org/socat/doc/socat.html#ADDRESS_VSOCK_LISTEN&#34;&gt;VSOCK-LISTEN&lt;/a&gt;:&lt;code&gt;&amp;lt;port&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Listen on &lt;code&gt;port&lt;/code&gt; and accepts a VSOCK connection.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.dest-unreach.org/socat/doc/socat.html#ADDRESS_VSOCK_CONNECT&#34;&gt;VSOCK-CONNECT&lt;/a&gt;:&lt;code&gt;&amp;lt;cid&amp;gt;:&amp;lt;port&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Establishes a VSOCK stream connection to the specified &lt;code&gt;cid&lt;/code&gt; and &lt;code&gt;port&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fosdem-2021&#34;&gt;FOSDEM 2021&lt;/h2&gt;
&lt;p&gt;If you are interested on VSOCK, I&amp;rsquo;ll talk witn Andra Paraschiv (AWS) about it
at FOSDEM 2021.
The talk is titled
&lt;a href=&#34;https://fosdem.org/2021/schedule/event/vai_virtio_vsock&#34;&gt;Leveraging virtio-vsock in the cloud and containers&lt;/a&gt;
and it&amp;rsquo;s scheduled for Saturday, February 6th 2021 at 11:30 AM (CET).&lt;/p&gt;
&lt;p&gt;We will show cool VSOCK use cases and some demos about developing, debugging,
and measuring the VSOCK performance, including &lt;code&gt;socat&lt;/code&gt; demos.&lt;/p&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;socat&lt;/code&gt; could be very useful for concatenating and redirecting sockets.
In this section we will see some examples.&lt;/p&gt;
&lt;p&gt;Examples below refer to a guest with &lt;code&gt;CID 42&lt;/code&gt; that we created using
&lt;a href=&#34;https://libguestfs.org/virt-builder.1.html&#34;&gt;virt-builder&lt;/a&gt;
and
&lt;a href=&#34;https://virt-manager.org/&#34;&gt;virt-install&lt;/a&gt;
.&lt;/p&gt;
&lt;h3 id=&#34;vm-setup&#34;&gt;VM setup&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;virt-builder&lt;/code&gt; is able to download the installer and create the disk image
with Fedora 33 or other distros.
It is also able to set the root password and inject the ssh public key,
simplifying the creation of guest disk image:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;VM_IMAGE&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;vsockguest_f33.qcow2&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ virt-builder --root-password&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;password:mypassword &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --ssh-inject root:file:/home/user/.ssh/id_rsa.pub &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --output&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;VM_IMAGE&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --format&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2 --size 10G --selinux-relabel &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --update fedora-33
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the disk image is ready, we create our VM with &lt;code&gt;virt-install&lt;/code&gt;.
We can specify the VM settings like the number of vCPUs, the amount of RAM,
and the &lt;code&gt;CID&lt;/code&gt; assigned to the VM [42]:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ virt-install --name vsockguest &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --ram &lt;span class=&#34;m&#34;&gt;2048&lt;/span&gt; --vcpus &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; --os-variant fedora33 &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --import --disk &lt;span class=&#34;nv&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;VM_IMAGE&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;,bus&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;virtio &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;        --graphics none --vsock cid.address&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;42&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After the creation of the VM, we will remain attached to the console and
we can detach from it by pressing &lt;code&gt;ctrl-]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can reattach to the console in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ virsh console vsockguest
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If the VM is turned off, we can boot it and attach directly to the console
in this way:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ virsh start --console vsockguest
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;ncat-like&#34;&gt;ncat like&lt;/h3&gt;
&lt;p&gt;It&amp;rsquo;s possible to use &lt;code&gt;socat&lt;/code&gt; like &lt;code&gt;ncat&lt;/code&gt;, transferring &lt;code&gt;stdin&lt;/code&gt; and &lt;code&gt;stdout&lt;/code&gt; via VSOCK.&lt;/p&gt;
&lt;h4 id=&#34;guest-listening&#34;&gt;Guest listening&lt;/h4&gt;
&lt;p&gt;In this example we start &lt;code&gt;socat&lt;/code&gt; in the guest listening on port &lt;code&gt;1234&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;guest$ socat - VSOCK-LISTEN:1234
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then we connect from the host using the &lt;code&gt;CID 42&lt;/code&gt; assigned to the VM:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ socat - VSOCK-CONNECT:42:1234
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point we can exchange characters between guest and host, since &lt;code&gt;stdin&lt;/code&gt;
and &lt;code&gt;stdout&lt;/code&gt; are linked through the VSOCK socket.&lt;/p&gt;
&lt;h4 id=&#34;host-listening&#34;&gt;Host listening&lt;/h4&gt;
&lt;p&gt;In this example we do the opposite, starting &lt;code&gt;socat&lt;/code&gt; in the host listening
on port &lt;code&gt;1234&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ socat - VSOCK-LISTEN:1234
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, in the guest, we connect to the host using the &lt;em&gt;well defined&lt;/em&gt; &lt;code&gt;CID 2&lt;/code&gt;.
It&amp;rsquo;s always used to reach the host:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;guest$ socat - VSOCK-CONNECT:2:1234
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;ssh-over-vsock&#34;&gt;ssh over VSOCK&lt;/h3&gt;
&lt;p&gt;The coolest feature of &lt;code&gt;socat&lt;/code&gt; is to concatenate sockets of different address
families, so in this example we redirect ssh traffic through VSOCK socket
exposed by the VM.&lt;/p&gt;
&lt;p&gt;This example could be useful if the VM doesn&amp;rsquo;t have any NIC attached and
we want to provide some network connectivity, like the ssh access.&lt;/p&gt;
&lt;p&gt;First of all, in the guest we start &lt;code&gt;socat&lt;/code&gt; linking the VSOCK socket listening on
port 22, to a TCP socket which will connect to the local TCP port 22 where the
ssh server is listening:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;guest$ socat VSOCK-LISTEN:22,reuseaddr,fork TCP:localhost:22
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;On the host we can use &lt;code&gt;socat&lt;/code&gt; as &lt;code&gt;ProxyCommand&lt;/code&gt; of &lt;code&gt;ssh&lt;/code&gt; to forward the connection
through the AF_VSOCK socket:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ ssh -o &lt;span class=&#34;s2&#34;&gt;&amp;#34;ProxyCommand socat - VSOCK-CONNECT:42:22&amp;#34;&lt;/span&gt; root@localhost
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;socat&lt;/code&gt; redirects all the traffic and allow us to use ssh over VSOCK to reach
the guest.&lt;/p&gt;
&lt;h3 id=&#34;connecting-sibling-vms&#34;&gt;Connecting sibling VMs&lt;/h3&gt;
&lt;p&gt;Another scenario where socat can be very useful is connecting two sibling VMs
running on the same host.&lt;/p&gt;
&lt;p&gt;Currently this is not supported by &lt;code&gt;vhost-vsock&lt;/code&gt;, so we can use &lt;code&gt;socat&lt;/code&gt; to
concatenate the two VMs.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see an example: suppose we launch another VM with &lt;code&gt;CID = 24&lt;/code&gt; on
the same host. Now we want to use &lt;code&gt;ncat&lt;/code&gt; in the VMs to communicate with each
other.&lt;/p&gt;
&lt;p&gt;Guest 42 will listen on port 1234 and guest 24 will initiate the connection.
But before we do this we need to set up the bridge in the host with &lt;code&gt;socat&lt;/code&gt; to
allow the two VMs to communicate:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;host$ socat VSOCK-LISTEN:1234 VSOCK-CONNECT:42:1234
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At this point we can launch &lt;code&gt;ncat&lt;/code&gt; in the VMs and communicate:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;guest_42$ nc --vsock -l &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note: as destination CID we have to use the host&amp;rsquo;s well-known CID (2) because
we need to connect to the &lt;code&gt;socat&lt;/code&gt; bridge running in the host, that will
redirect packets correctly between the two machines:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;guest_24$ nc --vsock &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>AF_VSOCK: nested VMs and loopback support available</title>
      <link>https://stefano-garzarella.github.io/posts/2020-02-20-vsock-nested-vms-loopback/</link>
      <pubDate>Tue, 25 Feb 2020 20:30:21 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2020-02-20-vsock-nested-vms-loopback/</guid>
      <description>&lt;p&gt;During the last
&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/&#34;&gt;KVM Forum 2019&lt;/a&gt;,
we discussed some next steps and several requests came from the audience.&lt;/p&gt;
&lt;p&gt;In the last months, we worked on that and recent Linux releases contain
interesting new features that we will describe in this blog post:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#nested-vms&#34;&gt;Nested VMs support&lt;/a&gt;, available in Linux 5.5&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#local-communication&#34;&gt;Local communication support&lt;/a&gt;, available in Linux 5.6&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;devconfcz-2020&#34;&gt;DevConf.CZ 2020&lt;/h2&gt;
&lt;p&gt;These updates and an introduction to AF_VSOCK were presented at
&lt;strong&gt;DevConf.CZ 2020&lt;/strong&gt; during the
&amp;ldquo;&lt;a href=&#34;https://devconfcz2020a.sched.com/event/YOwb/vsock-vm-host-socket-with-minimal-configuration&#34;&gt;VSOCK: VMâ†”host socket with minimal configuration&lt;/a&gt;&amp;rdquo; talk.
&lt;a href=&#34;https://static.sched.com/hosted_files/devconfcz2020a/b1/DevConf.CZ_2020_vsock_v1.1.pdf&#34;&gt;Slides&lt;/a&gt; and &lt;a href=&#34;https://youtu.be/R5DQWdPUQSY&#34;&gt;recording&lt;/a&gt; are available.&lt;/p&gt;
&lt;h2 id=&#34;nested-vms&#34;&gt;Nested VMs&lt;/h2&gt;
&lt;p&gt;Before Linux 5.5, the AF_VSOCK core supported only one transport loaded at
run time. That was a limit for nested VMs, because we need multiple transports
loaded together.&lt;/p&gt;
&lt;h3 id=&#34;types-of-transport&#34;&gt;Types of transport&lt;/h3&gt;
&lt;p&gt;Under the AF_VSOCK core, that provides the socket interface to the user space
applications, we have several transports that implement the communication
channel between guest and host.&lt;/p&gt;

&lt;link rel=&#34;stylesheet&#34; href=&#34;https://stefano-garzarella.github.io/css/hugo-easy-gallery.css&#34; /&gt;
&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_transports.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock transports&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;These transports depend on the hypervisor and we can put them in two groups:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H2G&lt;/strong&gt; (host to guest) transports: they run in the host and
usually they provide the device emulation; currently we have &lt;em&gt;vhost&lt;/em&gt; and &lt;em&gt;vmci&lt;/em&gt;
transports.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G2H&lt;/strong&gt; (guest to host) transports: they run in the guest and usually they are
device drivers; currently we have &lt;em&gt;virtio, vmci,&lt;/em&gt; and &lt;em&gt;hyperv&lt;/em&gt; transports.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;multi-transports&#34;&gt;Multi-transports&lt;/h3&gt;
&lt;p&gt;In a nested VM environment, we need to load both G2H and H2G transports
together in the L1 guest, for this reason, we implemented the multi-transports
support to use vsock through &lt;strong&gt;nested VMs&lt;/strong&gt;.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_nested_vms.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock and nested VMs&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Starting from Linux 5.5, the AF_VSOCK can handle two types of transports
loaded together at runtime:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;H2G&lt;/strong&gt; transport, to communicate with the guest&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;G2H&lt;/strong&gt; transport, to communicate with the host.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So in the QEMU/KVM environment, the L1 guest will load both &lt;em&gt;virtio-transport&lt;/em&gt;,
to communicate with L0, and &lt;em&gt;vhost-transport&lt;/em&gt; to communicate with L2.&lt;/p&gt;
&lt;h2 id=&#34;local-communication&#34;&gt;Local Communication&lt;/h2&gt;
&lt;p&gt;Another feature recently added is the possibility to communicate locally on
the same host.
This feature, suggested by &lt;a href=&#34;https://rwmj.wordpress.com/&#34;&gt;Richard WM Jones&lt;/a&gt;,
can be very useful for testing and debugging applications that use AF_VSOCK
without running VMs.&lt;/p&gt;
&lt;p&gt;Linux 5.6 introduces a new transport called &lt;em&gt;vsock-loopback&lt;/em&gt;, and a new well
know CID for local communication: &lt;strong&gt;VMADDR_CID_LOCAL (1)&lt;/strong&gt;.
Itâ€™s a special CID to direct packets to the same host that generated them.&lt;/p&gt;


&lt;div class=&#34;box&#34; style=&#34;max-width:300px&#34;&gt;
  &lt;figure  itemprop=&#34;associatedMedia&#34; itemscope itemtype=&#34;http://schema.org/ImageObject&#34;&gt;
    &lt;div class=&#34;img&#34;&gt;
      &lt;img itemprop=&#34;thumbnail&#34; src=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34; alt=&#34;/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34;/&gt;
    &lt;/div&gt;
    &lt;a href=&#34;https://stefano-garzarella.github.io/img/2020-02-20-vsock-nested-vms-loopback/vsock_loopback.png&#34; itemprop=&#34;contentUrl&#34;&gt;&lt;/a&gt;
      &lt;figcaption&gt;&lt;h4&gt;vsock loopback&lt;/h4&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Other CIDs can be used for the same purpose, but it&amp;rsquo;s preferable to use
VMADDR_CID_LOCAL:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Local Guest CID
&lt;ul&gt;
&lt;li&gt;if G2H is loaded (e.g. running in a VM)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VMADDR_CID_HOST (2)
&lt;ul&gt;
&lt;li&gt;if H2G is loaded and G2H is not loaded (e.g. running on L0).
If G2H is also loaded, then VMADDR_CID_HOST is used to reach the host&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Richard recently used the vsock local communication to implement a &lt;a href=&#34;https://github.com/libguestfs/nbdkit/commit/5e4745641bb4676f607fdb3f8750dbf6e9516877&#34;&gt;regression
test&lt;/a&gt;
test for &lt;a href=&#34;https://rwmj.wordpress.com/2019/10/21/nbd-over-af_vsock/&#34;&gt;nbdkit/libnbd vsock support&lt;/a&gt;, using the new &lt;a href=&#34;https://github.com/libguestfs/nbdkit/commit/01ce88f56f93afd577a8e3b2cc28d825693c8db2&#34;&gt;VMADDR_CID_LOCAL&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;example&#34;&gt;Example&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Listening on port 1234 using ncat(1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;l0$ nc --vsock -l &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Connecting to the local host using VMADDR_CID_LOCAL (1)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;l0$ nc --vsock &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1234&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;patches&#34;&gt;Patches&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1194660/&#34;&gt;[PATCH net-next v2 00/15] vsock: add multi-transports support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1207012/&#34;&gt;[PATCH net-next v2 0/6] vsock: add local transport support&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>QEMU 4.2 mmap(2)s kernel and initrd</title>
      <link>https://stefano-garzarella.github.io/posts/2019-12-22-qemu-kernel-initrd-mmapped/</link>
      <pubDate>Sun, 22 Dec 2019 17:46:44 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-12-22-qemu-kernel-initrd-mmapped/</guid>
      <description>&lt;p&gt;In order to save memory and boot time, &lt;strong&gt;QEMU 4.2&lt;/strong&gt; and later versions are able to
mmap(2) the kernel and initrd specified with &lt;code&gt;-kernel&lt;/code&gt; and &lt;code&gt;-initrd&lt;/code&gt; parameters.
This approach allows us to avoid reading and copying them into a buffer,
saving memory and time.&lt;/p&gt;
&lt;p&gt;The memory pages that contain kernel and initrd are shared between
multiple VMs using the same kernel and initrd images.
So, when many VMs are launched we can save memory by sharing pages, and save
time by avoiding reading them each time from the disk.&lt;/p&gt;
&lt;p&gt;This feature is automatically used for some targets with ELF kernels
(e.g. x86_64 vmlinux ELF image with
&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-08-23-qemu-linux-kernel-pvh/&#34;&gt;PVH entry point&lt;/a&gt;),
but it is not available with compressed kernel images (e.g. bzImage).&lt;/p&gt;
&lt;h2 id=&#34;patches&#34;&gt;Patches&lt;/h2&gt;
&lt;p&gt;The
&lt;a href=&#34;https://patchew.org/QEMU/20190724143105.307042-1-sgarzare@redhat.com/&#34;&gt;patches&lt;/a&gt;
that implement this feature are merged upstream and released with
&lt;a href=&#34;https://wiki.qemu.org/ChangeLog/4.2#Miscellaneous&#34;&gt;QEMU 4.2&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main change was to map kernel and initrd into the memory, instead of
reading them, using &lt;code&gt;g_mapped_file_*()&lt;/code&gt; APIs.&lt;/p&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;p&gt;We measured the memory footprint and the boot time using a standard QEMU
build (&lt;code&gt;qemu-system-x86_64&lt;/code&gt;) with a
&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-08-23-qemu-linux-kernel-pvh/&#34;&gt;PVH kernel&lt;/a&gt;
and initrd (cpio):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initrd size: 3.0M&lt;/li&gt;
&lt;li&gt;Kernel (vmlinux)
&lt;ul&gt;
&lt;li&gt;image size: 28M&lt;/li&gt;
&lt;li&gt;sections size [&lt;code&gt;size -A -d vmlinux&lt;/code&gt;]:  18.9M&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Julio Montes did a very good analysis and he posted the results here:
&lt;a href=&#34;https://www.mail-archive.com/qemu-devel@nongnu.org/msg633168.html&#34;&gt;https://www.mail-archive.com/qemu-devel@nongnu.org/msg633168.html&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;memory-footprint&#34;&gt;Memory footprint&lt;/h3&gt;
&lt;p&gt;We used &lt;a href=&#34;https://www.selenic.com/smem/&#34;&gt;smem&lt;/a&gt; to measure
&lt;a href=&#34;https://www.golinuxcloud.com/check-memory-usage-per-process-linux/&#34;&gt;USS and PSS&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;USS (Unique Set Size): amount of memory that is committed to physical memory
and is unique to a process; it is not shared with any other. It is the
amount of memory that would be freed if the process were to terminate.&lt;/li&gt;
&lt;li&gt;PSS (Proportional Set Size): This splits the accounting of shared pages that
are committed to physical memory between all the processes that have them
mapped.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;$ smem -k &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; grep &lt;span class=&#34;s2&#34;&gt;&amp;#34;PID\|&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;pidof qemu-system-x86_64&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  PID User     Command                         Swap      USS      PSS      RSS
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;24833&lt;/span&gt; qemu     /usr/bin/qemu-system-x86_64        &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;    71.6M    74.3M    105.2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is the memory footprint analysis, increasing the number of QEMU instances:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                           Memory footprint &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;MB&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     QEMU             before                 after
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;c1&#34;&gt;# instances        USS     PSS           USS     PSS&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;           102.0   105.8         102.3   106.2
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;            94.6   101.2          72.3    90.1
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;            94.1    98.0          72.0    81.5
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;m&#34;&gt;8&lt;/span&gt;            94.0    96.2          71.8    76.9
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;m&#34;&gt;16&lt;/span&gt;            93.9    95.1          71.6    74.3
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;boot-time&#34;&gt;Boot time&lt;/h3&gt;
&lt;p&gt;We measured the boot time using the
&lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time&#34;&gt;qemu-boot-time&lt;/a&gt;
scripts described in this
&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-08-24-qemu-linux-boot-time/&#34;&gt;post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This is the boot time analysis:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                   Boot &lt;span class=&#34;nb&#34;&gt;time&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;ms&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                          before                  after
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &lt;span class=&#34;c1&#34;&gt;# trace points&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end:           63.85                   55.91
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel:      82.11 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+18.26&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;          74.51 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+18.60&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user:       169.94 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+87.83&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;         159.06 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+84.56&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Mapping into memory the kernel and initrd images allowed us to save about 20 MB
of memory when multiple VMs are started and allowed us to speed up the boot by
about 10 milliseconds.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; both gains are strictly related to images size.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>KVM Forum 2019: virtio-vsock in QEMU, Firecracker and Linux</title>
      <link>https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/</link>
      <pubDate>Sat, 09 Nov 2019 18:45:25 +0100</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-11-08-kvmforum-2019-vsock/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://static.sched.com/hosted_files/kvmforum2019/50/KVMForum_2019_virtio_vsock_Andra_Paraschiv_Stefano_Garzarella_v1.3.pdf&#34;&gt;Slides&lt;/a&gt; and
&lt;a href=&#34;https://youtu.be/LFqz-VZPhFE&#34;&gt;recording&lt;/a&gt; are available for the &amp;ldquo;&lt;a href=&#34;https://sched.co/TmwK&#34;&gt;virtio-vsock in QEMU,
Firecracker and Linux: Status, Performance and Challenges&lt;/a&gt;&amp;rdquo;
talk that Andra Paraschiv and I presented at
&lt;a href=&#34;https://events19.linuxfoundation.org/events/kvm-forum-2019/&#34;&gt;KVM Forum 2019&lt;/a&gt;.
This was the 13th edition of the KVM Forum conference. It took place in Lyon,
France in October 2019.&lt;/p&gt;
&lt;p&gt;We talked about the current status and future works of VSOCK drivers in Linux
and how Firecracker and QEMU provides the virtio-vsock device.&lt;/p&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;Initially, Andra gave an overview of VSOCK, she described the state of the art,
and the key features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;it is very simple to configure, the host assigns an unique CID (Context-ID) to
each guest, and no configuration is needed inside the guest;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;it provides AF_VSOCK address family, allowing user space application in the
host and guest to communicate using standard POSIX Socket API (e.g.
bind, listen, accept, connect, send, recv, etc.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Andra also described common use cases for VSOCK, such as guest agents
(clipboard sharing, remote console, etc.), network applications using
SOCK_STREAM, and services provided by the hypervisor to the guests.&lt;/p&gt;
&lt;p&gt;Going into the implementation details, Andra explained how the device in the
guest communicates with the vhost backend in the host, exchanging data and
events (i.e. ioeventfd, irqfd).&lt;/p&gt;
&lt;h3 id=&#34;firecracker&#34;&gt;Firecracker&lt;/h3&gt;
&lt;p&gt;Focusing on Firecracker, Andra gave a brief overview on this new VMM (Virtual
Machine Monitor) written in Rust and she explained why, in the v0.18.0 release,
they switched from the experimental vhost-vsock implementation to a vhost-less
solution:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;focus on security impact&lt;/li&gt;
&lt;li&gt;less dependency on host kernel features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This change required a device emulation in Firecracker, that implements
virtio-vsock device model over MMIO. The device is exposed in the host using
UDS (Unix Domain Sockets).&lt;/p&gt;
&lt;p&gt;Andra described how Firecracker maps the VSOCK ports on the &lt;code&gt;uds_path&lt;/code&gt; specified
in the VM configuration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Host-Initiated Connections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Guest: create an AF_VSOCK socket and &lt;code&gt;listen()&lt;/code&gt; on PORT&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;connect()&lt;/code&gt; to AF_UNIX at &lt;code&gt;uds_path&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;send()&lt;/code&gt; &amp;ldquo;CONNECT PORT\n&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Guest: &lt;code&gt;accept()&lt;/code&gt; the new connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Guest-Initiated Connections&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Host: create and &lt;code&gt;listen()&lt;/code&gt; on an AF_UNIX socket at &lt;code&gt;uds_path_PORT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Guest: create an AF_VSOCK socket and &lt;code&gt;connect()&lt;/code&gt; to &lt;code&gt;HOST_CID&lt;/code&gt; and &lt;code&gt;PORT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host: &lt;code&gt;accept()&lt;/code&gt; the new connection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, she showed the performance of this solution, running
&lt;a href=&#34;https://github.com/stefano-garzarella/iperf-vsock&#34;&gt;iperf-vsock&lt;/a&gt; benchmark,
varying the size of the buffer used in Firecracker to transfer packets
between the virtio-vsock device and the UNIX domain socket. The throughput
on the guest to host path reaches 10 Gbps.&lt;/p&gt;
&lt;h3 id=&#34;qemu&#34;&gt;QEMU&lt;/h3&gt;
&lt;p&gt;In the second part of the talk, I described the QEMU implementation.
QEMU provides the virtio-vsock device using the vhost-vsock kernel module.&lt;/p&gt;
&lt;p&gt;The vsock device in QEMU handles only:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;configuration
&lt;ul&gt;
&lt;li&gt;user or management tool can configure the guest CID&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;live-migration
&lt;ul&gt;
&lt;li&gt;connected SOCK_STREAM sockets become disconnected.
Applications must handle a connection reset error and should reconnect.&lt;/li&gt;
&lt;li&gt;guest CID can be not available in the new host because can be
assigned to another VM. In this case the guest is notified about the
CID change.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The vhost-vsock kernel module handles the communication with the guest,
providing in-kernel virtio device emulation, to have very high performance and
to interface directly to the host socket layer.
In this way, also host application can directly use POSIX Socket API to
communicate with the guest. So, guest and host applications can be switched
between them, changing only the destination CID.&lt;/p&gt;
&lt;h3 id=&#34;virtio-vsock-linux-drivers&#34;&gt;virtio-vsock Linux drivers&lt;/h3&gt;
&lt;p&gt;After that, I told the story of VSOCK in the Linux tree, started in 2013
when the first implementation was merged, and the changes in the last year.&lt;/p&gt;
&lt;p&gt;These changes mainly regard fixes, but for the virtio/vhost transports we also
improved the performance with two simple changes released with Linux v5.4:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;reducing the number of credit update messages exchanged&lt;/li&gt;
&lt;li&gt;increasing the size of packets queued in the virtio-vsock device from 4 KB up
to 64 KB, the maximum packet size handled by virtio-vsock devices.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these changes we are able to reach ~40 Gbps in the Guest -&amp;gt; Host path,
because the guest can now send up to 64 KB packets directly to the host;
for the Host -&amp;gt; Guest path, we reached ~25 Gbps, because the host is still
using 4 KB buffer preallocated by the guest.&lt;/p&gt;
&lt;h3 id=&#34;tools-and-languages-that-support-vsock&#34;&gt;Tools and languages that support VSOCK&lt;/h3&gt;
&lt;p&gt;In the last few years, several applications, tools, and languages started to
support VSOCK and I listed them to update the audience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tools:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;wireshark &amp;gt;= 2.40 [2017-07-19]&lt;/li&gt;
&lt;li&gt;iproute2 &amp;gt;= 4.15 [2018-01-28]
&lt;ul&gt;
&lt;li&gt;ss&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tcpdump
&lt;ul&gt;
&lt;li&gt;merged in master [2019-04-16]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nmap &amp;gt;= 7.80 [2019-08-10]
&lt;ul&gt;
&lt;li&gt;ncat&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nbd
&lt;ul&gt;
&lt;li&gt;nbdkit &amp;gt;= 1.15.5 [2019-10-19]&lt;/li&gt;
&lt;li&gt;libnbd &amp;gt;= 1.1.6 [2019-10-19]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;iperf-vsock
&lt;ul&gt;
&lt;li&gt;iperf3 fork&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Languages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C
&lt;ul&gt;
&lt;li&gt;glibc &amp;gt;= 2.18 [2013-08-10]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Python
&lt;ul&gt;
&lt;li&gt;python &amp;gt;= 3.7 alpha 1 [2017-09-19]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Golang
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/mdlayher/vsock&#34;&gt;https://github.com/mdlayher/vsock&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rust
&lt;ul&gt;
&lt;li&gt;libc crate &amp;gt;= 0.2.59 [2019-07-08]
&lt;ul&gt;
&lt;li&gt;struct sockaddr_vm&lt;/li&gt;
&lt;li&gt;VMADDR_* macros&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nix crate &amp;gt;= 0.15.0 [2019-08-10]
&lt;ul&gt;
&lt;li&gt;VSOCK supported in the socket API (nix::sys::socket)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;next-steps&#34;&gt;Next steps&lt;/h3&gt;
&lt;p&gt;Concluding, I went through the next challenges that we are going to face:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;multi-transport&lt;/strong&gt; to use VSOCK in a nested VM environment. because we are
limited by the fact that the current implementation can handle only one
transport loaded at run time, so, we can&amp;rsquo;t load virtio_transport and
vhost_transport together in the L1 guest.
I already sent some patches upstream
[&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1168442/&#34;&gt;RFC&lt;/a&gt;,
&lt;a href=&#34;https://patchwork.ozlabs.org/cover/1181986/&#34;&gt;v1&lt;/a&gt;],
but they are still in progress.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;network namespace support&lt;/strong&gt; to create independent addressing domains with
VSOCK socket. This could be useful for partitioning VMs in different
domains or, in a nested VM environment, to isolate host applications
from guest applications bound to the same port.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;virtio-net as a transport for the virtio-vsock&lt;/strong&gt; to avoid to re-implement
features already done in virtio-net, such as mergeable buffers,
page allocation, small packet handling.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;from-the-audience&#34;&gt;From the audience&lt;/h4&gt;
&lt;p&gt;Other points to be addressed came from the comments we received from the
audience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;loopback device&lt;/strong&gt; could be very useful for developers to test applications
that use VSOCK socket. The current implementation support loopback only
in the guest, but it would be better to support it also in the host, adding
&lt;code&gt;VMADDR_CID_LOCAL&lt;/code&gt; special address.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;VM to VM communication&lt;/strong&gt; was asked by several people. Introducing it in the
VSOCK core could complicate the protocol, the addressing and could require
some sort of firewall.
For now we do not have in mind to do it, but I developed a simple user
space application to solve this issue:
&lt;a href=&#34;https://github.com/stefano-garzarella/vsock-bridge&#34;&gt;vsock-bridge&lt;/a&gt;.
In order to improve the performance of this solution, we will consider
the possibility to add &lt;code&gt;sendfile(2)&lt;/code&gt; or
&lt;a href=&#34;https://www.kernel.org/doc/html/v4.15/networking/msg_zerocopy.html&#34;&gt;MSG_ZEROCOPY&lt;/a&gt;
support to the AF_VSOCK core.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;virtio-vsock windows drivers&lt;/strong&gt; is not planned to be addressed, but contributions
are welcome. Other virtio windows drivers are available in the
&lt;a href=&#34;https://github.com/virtio-win/kvm-guest-drivers-windows&#34;&gt;vm-guest-drivers-windows&lt;/a&gt;
repository.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Stay tuned!&lt;/em&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>How to measure the boot time of a Linux VM with QEMU/KVM</title>
      <link>https://stefano-garzarella.github.io/posts/2019-08-24-qemu-linux-boot-time/</link>
      <pubDate>Sat, 24 Aug 2019 15:03:30 +0200</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-08-24-qemu-linux-boot-time/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time&#34;&gt;stefano-garzarella/qemu-boot-time&lt;/a&gt;
repository contains a Python perf-script and (Linux, QEMU, SeaBIOS) patches
to measure the boot time of a Linux VM with QEMU/KVM.&lt;/p&gt;
&lt;p&gt;Using I/O writes, we can trace events to measure the time consumed during the
boot phase by the different components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;firmware
&lt;ul&gt;
&lt;li&gt;SeaBIOS&lt;/li&gt;
&lt;li&gt;qboot&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;optionrom
&lt;ul&gt;
&lt;li&gt;linuxboot [bzImage]&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://stefano-garzarella.github.io/posts/2019-08-23-qemu-linux-kernel-pvh/&#34;&gt;pvh [vmlinux + PVH entry point]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We extended the I/O port addresses and values defined in
&lt;a href=&#34;https://github.com/bonzini/qboot/blob/master/benchmark.h&#34;&gt;qboot/benchmark.h&lt;/a&gt;
adding
&lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time/blob/master/benchmark.h&#34;&gt;new trace points to trace the kernel boot time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the repository you can find
&lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time#patches&#34;&gt;patches for Linux, QEMU, and SeaBIOS&lt;/a&gt; to add the I/O writes in the components involved during the boot,
and a &lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time/blob/master/perf-script/qemu-perf-script.py&#34;&gt;Python perf-script&lt;/a&gt; useful to process the data recorded through &lt;code&gt;perf&lt;/code&gt;
using perf-scriptâ€™s built-in Python interpreter.&lt;/p&gt;
&lt;h2 id=&#34;trace-points&#34;&gt;Trace points&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time/blob/master/benchmark.h&#34;&gt;benchmark.h&lt;/a&gt;
file contains the following trace points used in the &lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time/tree/master/patches&#34;&gt;patches&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;QEMU
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qemu_init_end&lt;/code&gt;: first kvm_entry (i.e. QEMU initialized has finished)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Firmware (SeaBIOS + optionrom or qboot)
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;fw_start&lt;/code&gt;: first entry of the firmware&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fw_do_boot&lt;/code&gt;: after the firmware initialization (e.g. PCI setup, etc.)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_boot&lt;/code&gt;: before the jump to the Linux kernel&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_pvhboot&lt;/code&gt;: before the jump to the Linux PVH kernel&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Linux Kernel
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;linux_start_kernel&lt;/code&gt;: first entry of the Linux kernel&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_user&lt;/code&gt;: before starting the init process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;custom-trace-points&#34;&gt;Custom trace points&lt;/h3&gt;
&lt;p&gt;If you want to add new trace points, you can simply add an I/O write to
&lt;code&gt;LINUX_EXIT_PORT&lt;/code&gt; or &lt;code&gt;FW_EXIT_PORT&lt;/code&gt; I/O port with a value (&amp;gt; 7) that
identifies the trace point:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nf&#34;&gt;outb&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LINUX_EXIT_PORT&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;);&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;perf script&lt;/code&gt; output will contain &lt;code&gt;Exit point 10&lt;/code&gt; line that identifies your
custom trace point:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 143.770419
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 143.964328 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.193909&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 164.71107 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+20.746742&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Exit point 10: 165.396804 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.685734&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 165.979486 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.582682&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 272.178335 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+106.198849&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;how-to-use&#34;&gt;How to use&lt;/h2&gt;
&lt;h3 id=&#34;clone-qemu-boot-time-repository&#34;&gt;Clone qemu-boot-time repository&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;~/repos&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/stefano-garzarella/qemu-boot-time.git
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;apply-patches-to-linux-qemu-and-seabios&#34;&gt;Apply patches to Linux, QEMU and SeaBIOS&lt;/h3&gt;
&lt;p&gt;Trace points are printed only if they are recorded, so you can enable few of
them, patching only the components that you are interested in.&lt;/p&gt;
&lt;h4 id=&#34;linux&#34;&gt;Linux&lt;/h4&gt;
&lt;p&gt;Apply the &lt;code&gt;patches/linux.patch&lt;/code&gt; to your Linux kernel in order to trace kernel
events&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/linux
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git checkout -b benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git am &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu-boot-time/patches/linux.patch
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;qemu&#34;&gt;QEMU&lt;/h4&gt;
&lt;p&gt;Apply the &lt;code&gt;patches/qemu.patch&lt;/code&gt; to your QEMU in order to trace optionrom
events&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git checkout -b benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git am &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu-boot-time/patches/qemu.patch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir build-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; build-benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;../configure --target-list&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;x86_64-softmmu ...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can use &lt;code&gt;qemu-system-x86_64 -L ${REPOS}/qemu/build-benchmark/pc-bios/optionrom/ ...&lt;/code&gt;
to use the optionrom patched.&lt;/p&gt;
&lt;h4 id=&#34;seabios&#34;&gt;SeaBIOS&lt;/h4&gt;
&lt;p&gt;Apply the &lt;code&gt;patches/seabios.patch&lt;/code&gt; to your SeaBIOS in order to trace bios
events&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/seabios
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git checkout -b benchmark
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git am &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu-boot-time/patches/seabios.patch
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make clean distclean
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;cp &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu/roms/config.seabios-256k .config
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make oldnoconfig
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can use &lt;code&gt;qemu-system-x86_64 -bios ${REPOS}/seabios/out/bios.bin ...&lt;/code&gt; to use
the SeaBIOS image patched.&lt;/p&gt;
&lt;h4 id=&#34;qboot&#34;&gt;qboot&lt;/h4&gt;
&lt;p&gt;qboot already defines trace points, we just need to compile it defining
&lt;code&gt;BENCHMARK_HACK&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qboot
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make clean
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;BIOS_CFLAGS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-DBENCHMARK_HACK=1&amp;#34;&lt;/span&gt; make
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can use &lt;code&gt;qemu-system-x86_64 -bios ${REPOS}/qboot/bios.bin ...&lt;/code&gt; to use the
qboot image.&lt;/p&gt;
&lt;h3 id=&#34;enable-kvm-events&#34;&gt;Enable KVM events&lt;/h3&gt;
&lt;p&gt;The following steps allow &lt;code&gt;perf record&lt;/code&gt; to get the kvm trace events:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &amp;gt; /sys/kernel/debug/tracing/events/kvm/enable
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; -1 &amp;gt; /proc/sys/kernel/perf_event_paranoid
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mount -o remount,mode&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;755&lt;/span&gt; /sys/kernel/debug
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mount -o remount,mode&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;755&lt;/span&gt; /sys/kernel/debug/tracing
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;record-the-trace-events&#34;&gt;Record the trace events&lt;/h3&gt;
&lt;p&gt;Start perf record to get the trace events&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;PERF_DATA&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;qemu_perf.data&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;perf record -a -e kvm:kvm_entry -e kvm:kvm_pio -e sched:sched_process_exec &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;            -o &lt;span class=&#34;nv&#34;&gt;$PERF_DATA&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;PERF_PID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can run QEMU multiple times to get also some statistics (Avg/Min/Max)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qemu-system-x86_64 -machine q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -bios seabios/out/bios.bin &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -L qemu/build-benchmark/pc-bios/optionrom/ &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -kernel linux/bzImage ...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qemu-system-x86_64 -machine q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -bios seabios/out/bios.bin &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -L qemu/build-benchmark/pc-bios/optionrom/ &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -kernel linux/bzImage ...
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qemu-system-x86_64 -machine q35,accel&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -bios seabios/out/bios.bin &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -L qemu/build-benchmark/pc-bios/optionrom/ &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;                   -kernel linux/bzImage ...
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Stop perf record&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;kill&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$PERF_PID&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;process-the-trace-recorded-using-the-qemu-perf-scriptpy&#34;&gt;Process the trace recorded using the qemu-perf-script.py&lt;/h3&gt;
&lt;p&gt;Note: times printed in milliseconds&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;perf script -s &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;REPOS&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;/qemu-boot-time/perf-script/qemu-perf-script.py -i &lt;span class=&#34;nv&#34;&gt;$PERF_DATA&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;in trace_begin
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sched__sched_process_exec     &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.435418353   &lt;span class=&#34;m&#34;&gt;289738&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_entry           &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.466887708   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.467070650   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.475818073   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.477168037   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.558779540   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55061.686849663   &lt;span class=&#34;m&#34;&gt;289741&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sched__sched_process_exec     &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.461869075   &lt;span class=&#34;m&#34;&gt;289793&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_entry           &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.496402472   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.496555385   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.505067184   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.506395502   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.584029910   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; 55067.704751791   &lt;span class=&#34;m&#34;&gt;289796&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sched__sched_process_exec     &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; 55070.073823767   &lt;span class=&#34;m&#34;&gt;289827&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_entry           &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; 55070.110507211   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; 55070.110694645   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55070.120092692   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf5, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55070.121437922   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55070.198628779   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;kvm__kvm_pio             &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; 55070.315734630   &lt;span class=&#34;m&#34;&gt;289830&lt;/span&gt; qemu-system-x86      &lt;span class=&#34;nv&#34;&gt;rw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;port&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;0xf4, &lt;span class=&#34;nv&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;count&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1, &lt;span class=&#34;nv&#34;&gt;val&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;6&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;in trace_end
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Trace qemu-system-x86
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; pid &lt;span class=&#34;m&#34;&gt;289738&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 31.469355
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 31.652297 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.182942&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 40.39972 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+8.747423&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 41.749684 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.349964&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 123.361187 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+81.611503&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 251.43131 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+128.070123&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; pid &lt;span class=&#34;m&#34;&gt;289793&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 34.533397
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 34.68631 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.152913&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 43.198109 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+8.511799&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 44.526427 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.328318&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 122.160835 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+77.634408&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 242.882716 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+120.721881&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;3&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; pid &lt;span class=&#34;m&#34;&gt;289827&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 36.683444
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 36.870878 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.187434&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 46.268925 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+9.398047&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 47.614155 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.34523&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 124.805012 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+77.190857&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 241.910863 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+117.105851&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Avg
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 34.228732
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 34.403161 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.174429&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 43.288918 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+8.885757&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 44.630088 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.34117&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 123.442344 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+78.812256&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 245.408296 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+121.965952&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Min
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 31.469355
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 31.652297 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.182942&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 40.39972 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+8.747423&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 41.749684 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.349964&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 122.160835 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+80.411151&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 241.910863 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+119.750028&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Max
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; qemu_init_end: 36.683444
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_start: 36.870878 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+0.187434&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; fw_do_boot: 46.268925 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+9.398047&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_boot: 47.614155 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+1.34523&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_kernel: 124.805012 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+77.190857&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; linux_start_user: 242.882716 &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;+118.077704&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>QEMU 4.0 boots uncompressed Linux x86_64 kernel</title>
      <link>https://stefano-garzarella.github.io/posts/2019-08-23-qemu-linux-kernel-pvh/</link>
      <pubDate>Fri, 23 Aug 2019 15:26:54 +0200</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-08-23-qemu-linux-kernel-pvh/</guid>
      <description>&lt;p&gt;&lt;strong&gt;QEMU 4.0&lt;/strong&gt; is now able to boot directly into the &lt;strong&gt;uncompressed Linux x86_64 kernel binary&lt;/strong&gt; with minimal firmware involvement using the &lt;strong&gt;PVH entry point&lt;/strong&gt; defined in the &lt;a href=&#34;https://xenbits.xen.org/docs/unstable/misc/pvh.html&#34;&gt;x86/HVM direct boot ABI&lt;/a&gt;. (&lt;code&gt;CONFIG_PVH=y&lt;/code&gt; must be enabled in the Linux config file).&lt;/p&gt;
&lt;p&gt;The x86/HVM direct boot ABI was initially developed for Xen guests, but with &lt;a href=&#34;#patches&#34;&gt;latest changes in both QEMU and Linux&lt;/a&gt;, QEMU is able to use that same entry point for booting KVM guests.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;QEMU &amp;gt;= 4.0&lt;/li&gt;
&lt;li&gt;Linux kernel &amp;gt;= 4.21
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CONFIG_PVH=y&lt;/code&gt; enabled&lt;/li&gt;
&lt;li&gt;&lt;code&gt;vmlinux&lt;/code&gt; uncompressed image&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how-to-use&#34;&gt;How to use&lt;/h2&gt;
&lt;p&gt;To boot the PVH kernel image, you can use the &lt;code&gt;-kernel&lt;/code&gt; parameter specifying the path to the &lt;code&gt;vmlinux&lt;/code&gt; image.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;qemu-system-x86_64 -machine q35,accel=kvm \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    -kernel /path/to/vmlinux \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    -drive file=/path/to/rootfs.ext2,if=virtio,format=raw \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    -append &amp;#39;root=/dev/vda console=ttyS0&amp;#39; -vga none -display none \
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    -serial mon:stdio
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;-initrd&lt;/code&gt; and &lt;code&gt;-append&lt;/code&gt; parameters are also supported as for compressed images.&lt;/p&gt;
&lt;h3 id=&#34;details&#34;&gt;Details&lt;/h3&gt;
&lt;p&gt;QEMU will automatically recognize if the &lt;code&gt;vmlinux&lt;/code&gt; image has the PVH entry point and it will use SeaBIOS with the new &lt;code&gt;pvh.bin&lt;/code&gt; optionrom to load the uncompressed image into the guest VM.&lt;/p&gt;
&lt;p&gt;As an alternative, &lt;a href=&#34;https://github.com/bonzini/qboot&#34;&gt;qboot&lt;/a&gt; can be used to load the PVH image.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Perf script and patches used to measure boot time: &lt;a href=&#34;https://github.com/stefano-garzarella/qemu-boot-time&#34;&gt;https://github.com/stefano-garzarella/qemu-boot-time&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The following values are expressed in milliseconds [ms]&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;QEMU (q35 machine) + SeaBIOS + bzImage&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qemu_init_end&lt;/code&gt;: 36.072056&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_kernel&lt;/code&gt;: 114.669522 (+78.597466)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_user&lt;/code&gt;: 191.748567 (+77.079045)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;QEMU (q35 machine) + SeaBIOS + vmlinux(PVH)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qemu_init_end&lt;/code&gt;: 51.588200&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_kernel&lt;/code&gt;: 62.124665 (+10.536465)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_user&lt;/code&gt;: 139.460582 (+77.335917)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;QEMU (q35 machine) + qboot + bzImage&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;qemu_init_end: 36.443638&lt;/li&gt;
&lt;li&gt;linux_start_kernel: 106.73115 (+70.287512)&lt;/li&gt;
&lt;li&gt;linux_start_user: 184.575531 (+77.844381)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;QEMU (q35 machine) + qboot + vmlinux(PVH)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;qemu_init_end: 51.877656&lt;/li&gt;
&lt;li&gt;linux_start_kernel: 56.710735 (+4.833079)&lt;/li&gt;
&lt;li&gt;linux_start_user: 133.808972 (+77.098237)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tracepoints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;qemu_init_end&lt;/code&gt;: first kvm_entry (i.e. QEMU initialization has finished)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_kernel&lt;/code&gt;: first entry of the Linux kernel (&lt;code&gt;start_kernel()&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;linux_start_user&lt;/code&gt;: before starting the init process&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;patches&#34;&gt;Patches&lt;/h2&gt;
&lt;p&gt;Linux patches merged upstream in Linux 4.21:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lkml.org/lkml/2018/12/14/1330&#34;&gt;https://lkml.org/lkml/2018/12/14/1330&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;QEMU patches merged upstream in QEMU 4.0:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mail-archive.com/qemu-devel@nongnu.org/msg587312.html&#34;&gt;https://www.mail-archive.com/qemu-devel@nongnu.org/msg587312.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.mail-archive.com/qemu-devel@nongnu.org/msg588561.html&#34;&gt;https://www.mail-archive.com/qemu-devel@nongnu.org/msg588561.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;qboot patches merged upstream:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bonzini/qboot/pull/17&#34;&gt;https://github.com/bonzini/qboot/pull/17&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bonzini/qboot/pull/18&#34;&gt;https://github.com/bonzini/qboot/pull/18&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>iperf3-vsock: how to measure VSOCK performance</title>
      <link>https://stefano-garzarella.github.io/posts/2019-08-22-vsock-iperf3/</link>
      <pubDate>Thu, 22 Aug 2019 17:52:06 +0200</pubDate>
      <author>sgarzare@redhat.com (Stefano Garzarella)</author>
      <guid>https://stefano-garzarella.github.io/posts/2019-08-22-vsock-iperf3/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://github.com/stefano-garzarella/iperf-vsock&#34;&gt;iperf-vsock&lt;/a&gt; repository
contains few patches to add the support of VSOCK address family to &lt;code&gt;iperf3&lt;/code&gt;.
In this way &lt;code&gt;iperf3&lt;/code&gt; can be used to measure the performance between guest and
host using VSOCK sockets.&lt;/p&gt;
&lt;p&gt;The VSOCK address family facilitates communication between virtual
machines and the host they are running on.&lt;/p&gt;
&lt;p&gt;To test VSOCK sockets (only Linux), you must use the new option &lt;code&gt;--vsock&lt;/code&gt; on
both server and client.
Other iperf3 options (e.g. &lt;code&gt;-t, -l, -P, -R, --bidir&lt;/code&gt;) are well supported by
VSOCK tests.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Linux host kernel &amp;gt;= 4.8&lt;/li&gt;
&lt;li&gt;Linux guest kernel &amp;gt;= 4.8&lt;/li&gt;
&lt;li&gt;QEMU &amp;gt;= 2.8&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;build-iperf3-vsock-from-source&#34;&gt;Build iperf3-vsock from source&lt;/h2&gt;
&lt;h3 id=&#34;clone-repository&#34;&gt;Clone repository&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;git clone https://github.com/stefano-garzarella/iperf-vsock
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; iperf-vsock
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;building&#34;&gt;Building&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;mkdir build
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; build
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;../configure
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(Note: If configure fails, try running &lt;code&gt;./bootstrap.sh&lt;/code&gt; first)&lt;/p&gt;
&lt;h2 id=&#34;example-with-fedora-30-host-and-guest&#34;&gt;Example with Fedora 30 (host and guest):&lt;/h2&gt;
&lt;h3 id=&#34;host-start-the-vm&#34;&gt;Host: start the VM&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;GUEST_CID&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo modprobe vhost_vsock
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo qemu-system-x86_64 -m 1G -smp &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; -cpu host -M &lt;span class=&#34;nv&#34;&gt;accel&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;kvm	&lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;     -drive &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;virtio,file&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/path/to/fedora.img,format&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;qcow2     &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;     -device vhost-vsock-pci,guest-cid&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;GUEST_CID&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;guest-start-iperf-server&#34;&gt;Guest: start iperf server&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# SELinux can block you, so you can write a policy or temporally disable it&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;sudo setenforce &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;iperf-vsock/build/src/iperf3 --vsock -s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;host-start-iperf-client&#34;&gt;Host: start iperf client&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;iperf-vsock/build/src/iperf3 --vsock -c &lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;GUEST_CID&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;output&#34;&gt;Output&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Connecting to host 3, port &lt;span class=&#34;m&#34;&gt;5201&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;local&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; port &lt;span class=&#34;m&#34;&gt;4008596529&lt;/span&gt; connected to &lt;span class=&#34;m&#34;&gt;3&lt;/span&gt; port &lt;span class=&#34;m&#34;&gt;5201&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; ID&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Interval           Transfer     Bitrate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   0.00-1.00   sec  1.30 GBytes  11.2 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   1.00-2.00   sec  1.67 GBytes  14.3 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   2.00-3.00   sec  1.57 GBytes  13.5 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   3.00-4.00   sec  1.49 GBytes  12.8 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   4.00-5.00   sec   &lt;span class=&#34;m&#34;&gt;971&lt;/span&gt; MBytes  8.15 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   5.00-6.00   sec  1.01 GBytes  8.71 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   6.00-7.00   sec  1.44 GBytes  12.3 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   7.00-8.00   sec  1.62 GBytes  13.9 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   8.00-9.00   sec  1.61 GBytes  13.8 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   9.00-10.00  sec  1.63 GBytes  14.0 Gbits/sec
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;- - - - - - - - - - - - - - - - - - - - - - - - -
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; ID&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt; Interval           Transfer     Bitrate
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   0.00-10.00  sec  14.3 GBytes  12.3 Gbits/sec                  sender
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt;  5&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;   0.00-10.00  sec  14.3 GBytes  12.3 Gbits/sec                  receiver
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;iperf Done.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
